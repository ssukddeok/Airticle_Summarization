{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets gradio pyngrok"
      ],
      "metadata": {
        "id": "AmbWlpuy4weM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from datasets import load_dataset\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, pipeline"
      ],
      "metadata": {
        "id": "hpIzhx3b40gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 로드 (CNN_dailymail 1000개 샘플 데이터 사용)\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:1000]\")\n",
        "print(dataset[0])  # 첫 번째 데이터 확인"
      ],
      "metadata": {
        "id": "-ogA60EkQH7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BART 모델과 토크나이저 로드\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "RdavTMci5EEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 요약 함수 정의\n",
        "def summarize(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(inputs.input_ids, max_length=130, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "uRfGGm78QmYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트: 첫 번째 뉴스 기사 요약\n",
        "example_text0 = dataset[0][\"article\"]\n",
        "print(\"Original Article0:\", example_text0)\n",
        "print(\"Summary:\", summarize(example_text0))\n"
      ],
      "metadata": {
        "id": "PSmYQsmjlwmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 마지막 뉴스 기사 요약\n",
        "example_text999 = dataset[999][\"article\"]\n",
        "print(\"Original Article999:\", example_text999)\n",
        "print(\"Summary:\", summarize(example_text999))\n"
      ],
      "metadata": {
        "id": "blGVstYtQq8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DistilBERT 기반 질의응답 파이프라인 로드\n",
        "qa_model = pipeline(\"question-answering\", model=\"distilbert-base-uncased\", tokenizer=\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "mZl7f-wT5Lw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 질의응답 함수 정의\n",
        "def answer_question(context, question):\n",
        "    result = qa_model(question=question, context=context)\n",
        "    return result[\"answer\"]\n",
        ""
      ],
      "metadata": {
        "id": "aNACJ3KfRBKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트: 요약문 기반 질문 응답1 (메인 주제 질문)\n",
        "question = \"What is the main topic of the article?\"\n",
        "context = summarize(example_text0)\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", answer_question(context, question))\n"
      ],
      "metadata": {
        "id": "JZSFAYrinIqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트: 요약문 기반 질문 응답 (인물에 대한 질문)\n",
        "question = \"Who is the main person mentioned in the article?\"\n",
        "context = summarize(example_text0)\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", answer_question(context, question))\n"
      ],
      "metadata": {
        "id": "Y264_VjFRPA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 질문과 상관없이 동일한 답을 내는 문제점을 발견\n",
        "원인과 해결 방안\n",
        "- 1. 요약 내용이 너무 간단한 경우 -> answer_question 함수를 fine-tuning해 요악함수를 정확하게 만들기.\n",
        "- 2. summarize 함수의 개선 -> summarize 함수의 출력 길이 늘리기.\n",
        "- 3. 질문에 맞는 답을 하도록 질문 -> 특정한 질문에 맞게 학습시키기."
      ],
      "metadata": {
        "id": "v2py_KjLSO4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer_question 함수를 fine-tuning.\n",
        "1. SQuAD 데이터 셋으로 모델 학습시키기\n",
        "question-answering 모델을 질문-답변(QnA) 작업에 적합하고, QnA 모델의 성능을 개선하기위해 SQuAD데이터 셋으로 질문에 대한 답변 정확도 높히기."
      ],
      "metadata": {
        "id": "CQc0rwPCUCHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForQuestionAnswering, BertTokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 데이터셋 불러오기\n",
        "dataset = load_dataset('squad')\n",
        "\n",
        "# 모델과 토크나이저 준비\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# 데이터셋을 모델에 맞게 준비\n",
        "def preprocess_function(examples):\n",
        "    # 토큰화 및 입력 길이 설정\n",
        "    tokenized_examples = tokenizer(examples['question'], examples['context'], truncation=True, padding='max_length', max_length=512)\n",
        "\n",
        "    # 시작 위치와 끝 위치 계산\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i in range(len(examples['answers'])):\n",
        "        # 정답의 첫 번째 텍스트 선택\n",
        "        answer = examples['answers'][i]['text'][0]  # 리스트에서 첫 번째 텍스트를 선택\n",
        "\n",
        "        # 각 문맥에 대해서 정답이 있는지 확인 (예시: 'context'는 리스트일 수 있음)\n",
        "        context = examples['context'][i]  # 각 문맥을 가져옴\n",
        "\n",
        "        # context 내에서 정답의 시작과 끝 위치를 찾기\n",
        "        start_pos = context.find(answer)\n",
        "        end_pos = start_pos + len(answer) - 1\n",
        "\n",
        "        # 만약 정답이 문맥에서 발견되지 않으면 (예를 들어, 잘못된 데이터가 있는 경우) -1로 설정\n",
        "        if start_pos == -1:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            start_positions.append(start_pos)\n",
        "            end_positions.append(end_pos)\n",
        "\n",
        "    tokenized_examples['start_positions'] = start_positions\n",
        "    tokenized_examples['end_positions'] = end_positions\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "train_dataset = dataset['train'].map(preprocess_function, batched=True)\n",
        "eval_dataset = dataset['validation'].map(preprocess_function, batched=True)\n",
        "\n",
        "# 훈련 인자 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    disable_tqdm=False,  # TQDM 활성화\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "# Fine-tuning 시작\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "vl79axUXT8Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# fine-tuning된 모델 로드\n",
        "qa_model = BertForQuestionAnswering.from_pretrained('./results')\n",
        "\n",
        "# QnA 파이프라인 설정\n",
        "qa_pipeline = pipeline(\"question-answering\", model=qa_model, tokenizer=tokenizer)\n",
        "\n",
        "# 질문 및 문맥\n",
        "context = \"Harry Potter star Daniel Radcliffe turns 18 and gains access to a £20 million fortune.\"\n",
        "question = \"What is Daniel Radcliffe's fortune?\"\n",
        "\n",
        "# 답변 추출\n",
        "answer = qa_pipeline(question=question, context=context)\n",
        "print(f\"Answer: {answer['answer']}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "x2Ytkx1XTkHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델과 토크나이저 로드\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "qa_model = pipeline(\"question-answering\", model=\"distilbert-base-uncased\", tokenizer=\"distilbert-base-uncased\")\n",
        "\n",
        "# 요약 함수\n",
        "def summarize(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(inputs.input_ids, max_length=130, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# 질문에 대한 답변 함수\n",
        "def answer_question(context, question):\n",
        "    result = qa_model(question=question, context=context)\n",
        "    return result[\"answer\"]\n",
        "\n",
        "# Gradio 인터페이스 생성\n",
        "def process(text, question):\n",
        "    summary = summarize(text)  # 텍스트 요약\n",
        "    answer = answer_question(summary, question)  # 요약된 텍스트에 대한 질문 응답\n",
        "    return summary, answer  # 두 가지 결과 반환\n",
        "\n",
        "# Gradio 인터페이스 설정\n",
        "iface = gr.Interface(fn=process,\n",
        "                     inputs=[\"text\", \"text\"],  # 두 개의 입력: 기사 텍스트, 질문 텍스트\n",
        "                     outputs=[\"text\", \"text\"],  # 두 개의 출력: 요약, 답변\n",
        "                     live=True)\n",
        "\n",
        "iface.launch()\n",
        "\n"
      ],
      "metadata": {
        "id": "j_-bY3KBN1Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SA4dt-euFDSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f7pYKSrLFFIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "시도1) 한국어버젼"
      ],
      "metadata": {
        "id": "JncPElX-EvcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 공개된 한국어 BERT 모델 사용\n",
        "model_name = \"kykim/bert-kor-base\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# 한국어 텍스트 요약 함수\n",
        "def summarize_korean(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    # 요약을 위해 모델을 적합하게 변경할 필요가 있을 수 있음\n",
        "    summary_ids = model.generate(inputs.input_ids, max_length=130, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Gradio 인터페이스 구성 (한국어)\n",
        "import gradio as gr\n",
        "\n",
        "def process(text, question):\n",
        "    summary = summarize_korean(text)  # 한국어 텍스트 요약\n",
        "    answer = answer_question(summary, question)  # 요약된 텍스트에 대한 질문 응답\n",
        "    return summary, answer  # 두 가지 결과 반환\n",
        "\n",
        "iface = gr.Interface(fn=process,\n",
        "                     inputs=[\"text\", \"text\"],\n",
        "                     outputs=[\"text\", \"text\"],\n",
        "                     live=True)\n",
        "\n",
        "iface.launch()\n",
        "\n"
      ],
      "metadata": {
        "id": "u4hJR47vSzgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "og0nzMmzS_5B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}